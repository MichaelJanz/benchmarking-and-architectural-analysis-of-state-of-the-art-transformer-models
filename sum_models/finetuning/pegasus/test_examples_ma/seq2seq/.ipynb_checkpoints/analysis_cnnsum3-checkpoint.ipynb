{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"pseudo\"\n",
    "window_size = 0\n",
    "overlap = 0\n",
    "sample = 2\n",
    "\n",
    "prefix=f\"{model_name.replace('/','')}_sample_{sample}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RevPrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RevPrep.Generation.generate.base import generate , get_model_tokenizer\n",
    "from RevPrep.Evaluations.scoring import analyse_sentiment, df_sentiment_model, df_sentiment_review, df_sentiments_model, df_sentiments_review, cal_rouge, cal_bert_score, import_bert_scores, cal_readability\n",
    "from RevPrep.plotting import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import glob\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_big = pkl.load(open(f\"cnndm_sample{sample}_big.pkl\",\"rb\"))\n",
    "sample_small = pkl.load(open(f\"cnndm_sample{sample}_small.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n",
      "TensorFlow version 2.3.1 available.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Texts done', max=300.0, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_window = generate(model_name, sample_big, model, tokenizer, True, window_size, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(results_window, open(f\"{prefix}.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RevPrep.Evaluations.scoring import cal_compression_rate\n",
    "from RevPrep.Helpers.helpers import text_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_big = [len(text_to_words(i)) for i in sample_big]\n",
    "len_short = [len(text_to_words(i)) for i in sample_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_window = {}\n",
    "results_window[\"generated summaries\"] = sample_small\n",
    "results_window[\"source_texts\"] = sample_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09016520821990145"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_compression_rate(len_big, len_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RevPrep.plotting.plot' from '/mnt/01D64EB52A75D220/Users/Science/Documents/Projekte/MA/sum_models/pegasus/pegasus/transformers/examples/seq2seq/RevPrep/plotting/plot.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(RevPrep.Evaluations.scoring)\n",
    "importlib.reload(RevPrep.plotting.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n",
      "TensorFlow version 2.3.1 available.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc2b7e7364849969cbb3fd1fd75ac0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9651d1eaf44c01b062fa14bffbece2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sens = analyse_sentiment(results_window, 5,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Means, created by the model of each summary gen and original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_sentiment_means([sens[5], sens[12], sens[17], sens[22]], first_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the sentiment function of each summary gen and original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_sentiment_functions([sens[5], sens[12], sens[17], sens[22]], first_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the progress of the mean squared error, which shows the deviation of both functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_sentiment_mae([sens[5], sens[12], sens[17], sens[22]], first_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print how each summary has been sentimented for gen and original and take the mean of these values. This is an indicator, whether the generated review has the same sentiment as the original in average (without information if the different sentiments are well pictured, for this, see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiments_review(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Sum of all Mse-means and Mse-Variances, which will be used for comparing to other models how well the progress of the sentiment is pictured by the generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiments_model(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rouge, std_dev_rouge, _ = cal_rouge(results_window[\"generated summaries\"], results_window[\"source_texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_rouge)\n",
    "print(std_dev_rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert-Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_bert_score(results_window[\"generated summaries\"], results_window[\"source_texts\"], prefix=prefix)\n",
    "mean_bs, stddev_bs, _ = import_bert_scores(prefix=prefix)\n",
    "print(mean_bs)\n",
    "print(stddev_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_rb, std_dev_rb = cal_readability(results_window[\"generated summaries\"], results_window[\"source_texts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Summary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_orig_avg = df_sentiments_review(sens).agg(\"mean\")[\"means_orig\"]\n",
    "mean_gen_avg = df_sentiments_review(sens).agg(\"mean\")[\"means_gen\"]\n",
    "mean_difference = df_sentiments_review(sens).agg(\"mean\")[\"difference\"]\n",
    "stddev_difference = df_sentiments_review(sens).agg(\"std\")[\"difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mae = df_sentiments_model(sens)[\"sum of mae-mean\"]\n",
    "sum_std_dev_mae_means = df_sentiments_model(sens)[\"Sum of mae-std-dev\"]\n",
    "mean_mae = df_sentiments_model(sens)[\"mae mean\"]\n",
    "std_dev_mae_means = df_sentiments_model(sens)[\"mae std dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1pm = mean_rouge[\"r1-p\"]\n",
    "r1rm = mean_rouge[\"r1-r\"]\n",
    "r1fm = mean_rouge[\"r1-f1\"]\n",
    "\n",
    "r2pm = mean_rouge[\"r2-p\"]\n",
    "r2rm = mean_rouge[\"r2-r\"]\n",
    "r2fm = mean_rouge[\"r2-f1\"]\n",
    "\n",
    "rlpm = mean_rouge[\"rl-p\"]\n",
    "rlrm = mean_rouge[\"rl-r\"]\n",
    "rlfm = mean_rouge[\"rl-f1\"]\n",
    "\n",
    "r1ps = std_dev_rouge[\"r1-p\"]\n",
    "r1rs = std_dev_rouge[\"r1-r\"]\n",
    "r1fs = std_dev_rouge[\"r1-f1\"]\n",
    "\n",
    "r2ps = std_dev_rouge[\"r2-p\"]\n",
    "r2rs = std_dev_rouge[\"r2-r\"]\n",
    "r2fs = std_dev_rouge[\"r2-f1\"]\n",
    "\n",
    "rlps = std_dev_rouge[\"rl-p\"]\n",
    "rlrs = std_dev_rouge[\"rl-r\"]\n",
    "rlfs = std_dev_rouge[\"rl-f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bspm = mean_bs[\"p\"]\n",
    "bsrm = mean_bs[\"r\"]\n",
    "bsfm = mean_bs[\"f1\"]\n",
    "\n",
    "bsps = stddev_bs[\"p\"]\n",
    "bsrs = stddev_bs[\"r\"]\n",
    "bsfs = stddev_bs[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r_m = {\"mR1-p\": r1pm, \"mR1-r\": r1rm, \"mR1-f1\": r1fm, \"mR2-p\":r2pm, \"mR2-r\": r2rm,\" mR2-f1\": r2fm, \"mRl-p\": rlpm, \"mRl-r\": rlrm, \"mRl-f1\": rlfm,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r_s = {\"sR1-p\": r1ps, \"sR1-r\": r1rs, \"sR1-f1\": r1fs, \"sR2-p\":r2ps, \"sR2-r\": r2rs, \"sR2-f1\": r2fs, \"sRl-p\": rlps, \"sRl-r\": rlrs, \"sRl-f1\": rlfs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bs = {\"bs-mean F1\": bsfm, \"bs-mean P\": bspm, \"bs-mean R\": bsrm, \"bs-std-dev F1\": bsfs, \"bs-std-dev P\": bsps, \"bs-std-dev R\": bsrs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment = {\"Sen. rev. mean orig\": mean_orig_avg, \"Sen. rev. mean gen\": mean_gen_avg, \"Sen. rev. diff mean\": mean_difference, \\\n",
    "                  \"Sen. rev. diff stddev\": stddev_difference,\n",
    "                  \"Sen. mae sum\": sum_mae, \"Sen. mae std.\": sum_std_dev_mae_means, \"Sen. mae avg\": mean_mae, \"Sen mae std dev avg\": std_dev_mae_means}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sen = pd.DataFrame(data=data_sentiment, index=[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rouge_mean = pd.DataFrame(data=data_r_m, index=[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rouge_std_dev = pd.DataFrame(data=data_r_s, index=[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bert = pd.DataFrame(data=data_bs, index=[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rb_means = pd.DataFrame(data=means_rb, index=[model_name])\n",
    "results_rb_std_dev = pd.DataFrame(data=std_dev_rb, index=[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(results_sen, open(f\"benchmark_results/sentimentanalysis/{prefix}.pkl\", \"wb\"))\n",
    "pkl.dump(results_rouge_mean, open(f\"benchmark_results/rougemean/{prefix}.pkl\", \"wb\"))\n",
    "pkl.dump(results_rouge_std_dev, open(f\"benchmark_results/rougestddev/{prefix}.pkl\", \"wb\"))\n",
    "pkl.dump(results_bert, open(f\"benchmark_results/bertscore/{prefix}.pkl\", \"wb\"))\n",
    "pkl.dump(results_rb_means, open(f\"benchmark_results/readabilitymean/{prefix}.pkl\", \"wb\"))\n",
    "pkl.dump(results_rb_std_dev, open(f\"benchmark_results/readabilitystddev/{prefix}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rouge_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rouge_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rb_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rb_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tr_pegasus_env)",
   "language": "python",
   "name": "tr_pegasus_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
